---
title: "Exploring historic data"
author: "Vanuatu National Statistics Office"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    toc_float: yes
params: 
  password: "ThisIsNotMyPassword"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r preparation, include=FALSE}

# Load the required libraries
library(RMySQL) # Interactiing with MySQL from R
library(knitr) # Nice tables
library(kableExtra) # Extra nice tables
library(basicPlotteR) # Progress bar
library(plotly) # Interactive graphs
library(openxlsx) # REad and write excel formatted files

# Set the number of digits to use before using exp notation
options("scipen"=50)

# Load the general R functions
source("functions.R")

# Get the repository directory - for building relative paths
repository <- dirname(getwd())
```

# Introduction

Here we document the exploration of the historic trade statistics data for Vanuatu. Our aim is to understand how the value of different imported and exported commodities has changed through time. Due to the large nature of the historic data, these data are stored on a [MySQL](https://www.mysql.com/) server. We will use the [RMySQL](https://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf) R package to interact with the local MySQL server from R.

To set up the local MySQL server, a dump of the VNSO server was shared and imported via the [MySQL Workbench](https://www.mysql.com/products/workbench/) on my computer. I had previously install MySQL by following these instructions:
- Installing on linux systems ([link](https://dev.mysql.com/doc/refman/8.0/en/linux-installation.html)), with extra help [here](https://itsfoss.com/install-mysql-ubuntu/).
- Installing on mac: ([link](https://dev.mysql.com/doc/mysql-osx-excerpt/5.7/en/osx-installation.html)), also, I think it comes ready installed! See [here](https://www.thoughtco.com/installing-mysql-on-mac-2693866))
- Installing on Windows ([link](https://dev.mysql.com/downloads/installer/)), with extra help [here](https://www.wikihow.com/Install-the-MySQL-Database-Server-on-Your-Windows-PC)

# Connecting to the database

The MySQL server is running locally and we can connect to it from R with the following code:

```{r connection, eval=FALSE}
# Open a connection to a MySQL database
connection <- dbConnect(MySQL(), 
                            user='JosephCrispell', 
                            password=readline(prompt="Enter password: "), # Doing this as password never stored in easily accessible format now
                            dbname='vnso',
                            host='localhost')
```


```{r echo=FALSE}
# Open a connection to a MySQL database
connection <- dbConnect(MySQL(), 
                        user='JosephCrispell', 
                        password=params$password, # Doing this as password never stored in easily accessible format now
                        dbname='vnso',
                        host='localhost')
```

```{r store local csv, eval=FALSE, echo=FALSE}
# Get the historic data
exports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_export_99_19")
imports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_import_99_19")

# Save the data as a standard csv
write.table(exports, file=file.path("..", "data", "secure", "tradeStats_historic_EXPORTS_14-09-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
write.table(imports, file=file.path("..", "data", "secure", "tradeStats_historic_IMPORTS_14-09-20.csv"),
            sep=",", row.names=FALSE, quote=TRUE)
```

```{r get size of tables, echo=FALSE}
nImportRecords <- dbGetQuery(conn=connection, statement="SELECT TABLE_ROWS FROM information_schema.TABLES WHERE table_name = 'historical_import_99_19'") # Slower more accurate query: SELECT COUNT(*) FROM historical_import_99_19
nExportRecords <- dbGetQuery(conn=connection, statement="SELECT TABLE_ROWS FROM information_schema.TABLES WHERE table_name = 'historical_export_99_19'") # Slower more accurate query: SELECT COUNT(*) FROM historical_export_99_19
```


On our local MySQL database there are two tables:

- `historical_export_99_19` - the historic data for all exports (~`r nExportRecords` rows)
- `historical_import_99_19` - the historic data for all imports (~`r nImportRecords` rows)

The imports data looks like this:

```{r quick look at historic imports data, echo=FALSE}

# Extract a small subset of historic data
historic_imports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_import_99_19 LIMIT 25")

# View the table
prettyTable(historic_imports)
```

<br>
And the exports data like this:
<br>

```{r quick look at historic exportsdata, echo=FALSE}

# Extract a small subset of historic data
historic_exports <- dbGetQuery(conn=connection, statement="SELECT * FROM historical_export_99_19 LIMIT 25")

# View the table
prettyTable(historic_exports)
```

# Summarise the `Value` distribution for each HS code

Each commodity in the historic data is identified by an 8 digit **HS** (Harmonized System) code. We are aiming to define the expected value distribution for each unique **HS** code through time for imports and exports. 

Firstly we need to create a column to store the correct `Value` column by the `Weight` column for both the imports and exports:
```{r create column to store value divded by weight}
# Add column to correct Value column by Weight into IMPORTS table
colNames_imports <- dbGetQuery(connection, statement="SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA='vnso' AND TABLE_NAME='historical_import_99_19'")
if("Value_corrected" %in% colNames_imports[, 1] == FALSE){
  dbSendQuery(connection, statement="ALTER TABLE historical_import_99_19 ADD Value_corrected DOUBLE AS (Value / Weight)")
}

# Add column to correct Value column by Weight into EXPORTS table
colNames_exports <- dbGetQuery(connection, statement="SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA='vnso' AND TABLE_NAME='historical_export_99_19'")
if("Value_corrected" %in% colNames_exports[, 1] == FALSE){
  dbSendQuery(connection, statement="ALTER TABLE historical_export_99_19 ADD Value_corrected DOUBLE AS (Value / Weight)")
}
```

With the `Value_corrected` column created, we can create some simple statistics:

```{r summarise corrected value column}
# Summarise the corrected value column for each commodity for each year
commoditySummary_exports <- dbGetQuery(conn=connection, statement="SELECT HS, YEAR, Description, CTY_Dest AS CTY, AVG(Value_corrected) AS average, MAX(Value_corrected) AS maximum, MIN(Value_corrected) AS minimum, COUNT(Value_corrected) AS count, SUM(Value_corrected) AS sum, STDDEV_SAMP(Value_corrected) AS sd, VAR_SAMP(Value_corrected) as variance FROM historical_export_99_19 GROUP BY HS, YEAR")
commoditySummary_imports <- dbGetQuery(conn=connection, statement="SELECT HS, YEAR, Description, CTY_Origin AS CTY, AVG(Value_corrected) AS average, MAX(Value_corrected) AS maximum, MIN(Value_corrected) AS minimum, COUNT(Value_corrected) AS count, SUM(Value_corrected) AS sum, STDDEV_SAMP(Value_corrected) AS sd, VAR_SAMP(Value_corrected) as variance FROM historical_import_99_19 GROUP BY HS, YEAR")

# Combine the summaries into a single table
commoditySummary_exports$Type <- "EXPORT"
commoditySummary_imports$Type <- "IMPORT"
commoditySummary <- rbind(commoditySummary_exports, commoditySummary_imports)

# Remove NA values
commoditySummary <- commoditySummary[is.na(commoditySummary$HS) == FALSE, ]

# Convert the year column to numeric
commoditySummary$YEAR <- as.numeric(commoditySummary$YEAR)

# Sort the commodity by HS code and year
commoditySummary <- commoditySummary[order(commoditySummary$HS, commoditySummary$YEAR), ]

# Remove any records before 1999
commoditySummary <- commoditySummary[commoditySummary$YEAR >= 1999, ]

# View the table
prettyTable(commoditySummary[1:25, ])
```

# Track trends in the principle imports and exports

```{r read in the principle commodities table}

# Read in the principle commodities table
principle <- read.xlsx(file.path(repository, "data", "open", "OPN_FINAL_ASY_PrincipleCommoditiesClassifications_31-01-20.xlsx"))

# Merge the principle definitions into the commodity summary tables
commoditySummary <- merge(commoditySummary, principle, by.x="HS", by.y="HS.Code")
```

```{r calculate summary statistics for principle commodities}

```


```{r quick visual, echo=FALSE, eval=FALSE}
library(basicPlotteR)
library(RColorBrewer)
# Make a quick visual
exportRows <- is.na(commoditySummary$PRINCIPAL.EXPORTS) == FALSE & commoditySummary$PRINCIPAL.EXPORTS != "OTHER PRODUCTS"
yLim <- range(commoditySummary[exportRows, "average"], na.rm=TRUE)
yLimLogged <- log(yLim)
xLim <- range(commoditySummary[exportRows, "YEAR"], na.rm=TRUE)
plot(x=NULL, y=NULL, xlim=xLim, ylim=yLimLogged, 
     bty="n", las=1, yaxt="n", ylab="Value", xlab="Year",
     main="Trends in principle exports through time")
at <- c(0.1, 10^(0:10))
at <- at[at < yLim[2]]
labels <- ifelse(at > 10000, paste0(at/1000000, "M"), at)
atLogged <- log(at)
axis(side=2, at=atLogged, labels=labels, las=1)

types <- unique(commoditySummary[commoditySummary$PRINCIPAL.EXPORTS != "OTHER PRODUCTS", "PRINCIPAL.EXPORTS"])
colours <- colorRampPalette(brewer.pal(8, "Dark2"))(length(types))
colours <- setAlpha(colours, 0.5)
for(i in seq_along(types)){
  
  subset <- commoditySummary[commoditySummary$PRINCIPAL.EXPORTS == types[i], ]
  subset <- subset[order(subset$YEAR), ]
  
  
  for(code in unique(subset$HS)){
    
    lines(x=subset[subset$HS == code, "YEAR"],
          y=log(subset[subset$HS == code, "average"]), 
          col=colours[i], lwd=2)
  }
}

legend(x=2019, y=16, legend=types, text.col=colours, bty="n", cex=0.75, pch=NA, xpd=TRUE)
```


# Closing a connection to the `MySQL` server

To wrap up, we can close our connection to the `MySQL` server using the following code:
```{r warning=FALSE}
dbDisconnect(conn=connection)
```

